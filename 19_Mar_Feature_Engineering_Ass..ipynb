{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b8629-e559-44a2-8f20-a7fdf2bef758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "Min-Max scaling is a data preprocessing technique used to rescale numerical features in a dataset to a specific range, typically between 0 and 1. It's done to ensure that all features have the same scale, which can be important for machine learning algorithms that are sensitive to the magnitude of input data. Here's a simple explanation of how it works:\n",
    "\n",
    "Find the minimum (min) and maximum (max) values of the feature you want to scale.\n",
    "\n",
    "For each data point in the feature, subtract the minimum value and then divide by the range (max - min).\n",
    "\n",
    "Mathematically, the formula for Min-Max scaling is:\n",
    "\n",
    "Scaled Value=Original Value - Min Value /Max Value-Min Value\n",
    "\n",
    "â€‹\n",
    " \n",
    "\n",
    "Here's an example to illustrate Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset of ages ranging from 20 to 60 years, and you want to scale this feature to a range between 0 and 1.\n",
    "\n",
    "Minimum age (min): 20 years\n",
    "Maximum age (max): 60 years\n",
    "Now, let's say you have an age value of 40 years that you want to scale:\n",
    "Scaled Age=40-20/60-20=20/40= 0.5\n",
    "\n",
    "\n",
    "So, the scaled value for an age of 40 years would be 0.5 after Min-Max scaling. \n",
    "This ensures that all the age values in your dataset are transformed to a common scale between 0 and 1, \n",
    "making it easier for machine learning algorithms to work with this feature, especially when you have multiple features with different scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab710dd-e3ae-4067-aad7-11ceaf6df47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6 0.8]\n"
     ]
    }
   ],
   "source": [
    "#Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?Provide an example to illustrate its application.\n",
    "\"\"\"Unit Vector scaling, also known as Normalization, is another data preprocessing technique used to scale numerical features in a dataset. \n",
    "   It differs from Min-Max scaling in that it scales the features so that their magnitudes (norms) are equal to 1.\n",
    "   This technique is useful when you want to retain the direction or angle information between data points but standardize their magnitudes.\n",
    "   Scaled Value= Original Value/Norm of the Vector\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# Sample vector\n",
    "vector = np.array([3, 4])\n",
    "\n",
    "# Calculate the norm\n",
    "norm = np.linalg.norm(vector)\n",
    "\n",
    "# Scale the vector\n",
    "unit_vector = vector / norm\n",
    "\n",
    "print(unit_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27a8537-dc47-4ecd-af34-197e86b310ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.49962554]\n",
      " [  2.50184501]\n",
      " [-11.50109608]\n",
      " [ 13.49887661]]\n"
     ]
    }
   ],
   "source": [
    "#Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\"\"\"\n",
    "Principal Component Analysis (PCA) is a widely used technique in statistics and data analysis for dimensionality reduction and feature extraction.\n",
    "It's used to simplify complex datasets by transforming them into a lower-dimensional representation while preserving as much of the original data's variance as possible.\n",
    "PCA accomplishes this by finding new orthogonal axes called principal components, which are linear combinations of the original features.\n",
    "\n",
    "Data Centering: PCA begins by centering the data, which means subtracting the mean from each feature. This ensures that the data is centered around the origin.\n",
    "\n",
    "Covariance Matrix Calculation: PCA calculates the covariance matrix of the centered data. The covariance matrix describes how features in the dataset vary together.\n",
    "\n",
    "Eigenvalue and Eigenvector Calculation: PCA then calculates the eigenvalues and eigenvectors of the covariance matrix. These eigenvectors represent the principal components, and the eigenvalues represent the variance explained by each principal component.\n",
    "\n",
    "Sorting Principal Components: The eigenvectors are sorted in descending order of their corresponding eigenvalues. The principal components are ranked by the amount of variance they explain, with the first principal component explaining the most variance, the second explaining the second most, and so on.\n",
    "\n",
    "Dimensionality Reduction: To reduce dimensionality, you can select a subset of the top-ranked principal components that collectively explain most of the variance in the data. These selected principal components form a new basis for the data.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[160, 60], [165, 65], [155, 55], [175, 70]])\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA(n_components=1)\n",
    "\n",
    "# Fit and transform the data to reduce it to 1 dimension\n",
    "reduced_data = pca.fit_transform(data)\n",
    "\n",
    "print(reduced_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6095e42c-55fa-490c-83b2-c39635b8ee84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-14.40108261   0.21522015]\n",
      " [  8.59843568   0.06636213]\n",
      " [-25.79530703  -0.19908638]\n",
      " [ 31.59795396  -0.0824959 ]]\n"
     ]
    }
   ],
   "source": [
    "#Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "\"\"\"Principal Component Analysis (PCA) can be thought of as a feature extraction technique, and its primary goal is to find new, uncorrelated features called principal components that are linear combinations of the original features. The relationship between PCA and feature extraction is that PCA extracts these new features while reducing the dimensionality of the data. Here's how PCA can be used for feature extraction:\n",
    "\n",
    "Dimensionality Reduction: PCA is often used to reduce the dimensionality of a dataset while retaining as much information as possible. It does this by finding the principal components, which are combinations of the original features that capture the most variance in the data. These principal components are used as the new, extracted features.\n",
    "\n",
    "Variance Retention: When you apply PCA for feature extraction, you typically select a subset of the top-ranked principal components that collectively explain a high percentage of the variance in the data. By retaining these components, you effectively reduce the number of features in your dataset while minimizing information loss.\n",
    "\n",
    "New Feature Space: The selected principal components form a new feature space. This space can be used for various purposes such as visualization, data analysis, or feeding into machine learning algorithms.\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Sample health data with 4 features (blood pressure, cholesterol, BMI, heart rate)\n",
    "data = np.array([[120, 200, 25, 70],\n",
    "                 [130, 220, 27, 75],\n",
    "                 [115, 190, 24, 68],\n",
    "                 [140, 240, 29, 80]])\n",
    "\n",
    "# Create a PCA object, retaining 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data to extract 2 features\n",
    "extracted_features = pca.fit_transform(data)\n",
    "\n",
    "print(extracted_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4180c775-b711-4c54-a845-4b2617bdc08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.         0.25      ]\n",
      " [1.         0.         0.625     ]\n",
      " [0.5        0.57142857 0.        ]\n",
      " [0.25       0.28571429 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "#contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data. \n",
    "\"\"\"To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, follow these steps:\n",
    "\n",
    "Understand Your Data: Start by understanding the dataset and the specific features you have. In your case, you mentioned that the dataset contains features like price, rating, and delivery time. Ensure you know the range and distribution of values for each of these features.\n",
    "\n",
    "Import Libraries: Import the necessary Python libraries, including NumPy and scikit-learn, for data preprocessing.\"\"\"\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data with three features: price, rating, delivery time\n",
    "data = np.array([[10, 4.5, 30],\n",
    "                 [30, 3.8, 45],\n",
    "                 [20, 4.2, 20],\n",
    "                 [15, 4.0, 60]])\n",
    "\n",
    "# Create a Min-Max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7360449-1964-417d-82a9-b592d280902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "#features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "# dimensionality of the dataset.\n",
    "\n",
    "\"\"\"Step 1: Data Preparation\n",
    "Before applying PCA, it's essential to prepare your data:\n",
    "\n",
    "Data Cleaning: Ensure that your dataset is clean, free of missing values, and outliers are handled appropriately.\n",
    "\n",
    "Feature Scaling: Normalize or standardize your data if necessary, so that all features have a similar scale. PCA is sensitive to the scale of the features.\n",
    "\n",
    "Step 2: Applying PCA\n",
    "\n",
    "Once your data is prepared, follow these steps to apply PCA:\n",
    "\n",
    "Center the Data: Subtract the mean from each feature. This centers the data around the origin.\n",
    "\n",
    "Calculate the Covariance Matrix: Compute the covariance matrix of the centered data. The covariance matrix provides information about how different features are related.\n",
    "\n",
    "Compute Eigenvectors and Eigenvalues: Calculate the eigenvectors and eigenvalues of the covariance matrix. These eigenvectors represent the principal components, and the eigenvalues represent the variance explained by each principal component.\n",
    "\n",
    "Select Principal Components: Sort the eigenvectors in descending order of their corresponding eigenvalues. The principal components with the highest eigenvalues explain the most variance in the data. You can choose how many principal components to retain based on how much variance you want to preserve. For instance, if you retain 90% of the variance, you can sum the eigenvalues and keep the corresponding eigenvectors until their cumulative sum reaches 90%.\n",
    "\n",
    "Transform the Data: Project your original data onto the selected principal components. This forms a new dataset with reduced dimensionality. The transformed data can be used for modeling.\n",
    "\n",
    "Step 3: Modeling and Evaluation\n",
    "\n",
    "After reducing the dimensionality of your dataset using PCA, you can proceed with building your stock price prediction model using the transformed data. It's essential to:\n",
    "\n",
    "Split your data into training and testing sets.\n",
    "\n",
    "Apply a regression or time-series forecasting model to the transformed data. Common models for stock price prediction include linear regression, ARIMA, or machine learning algorithms like random forests or neural networks.\n",
    "\n",
    "Evaluate your model's performance using appropriate metrics, such as mean squared error (MSE) or root mean squared error (RMSE), on the testing dataset.\n",
    "\n",
    "Step 4: Interpretation\n",
    "\n",
    "PCA can make your model more interpretable by highlighting the most important dimensions (principal components) of your data. You can analyze the loadings of the original features on these principal components to gain insights into which aspects of the financial and market data are most influential in predicting stock prices.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "206bad0a-42d7-44e2-845b-2a4cfaf3f5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "#Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "# values to a range of -1 to 1.\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the new range\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# Calculate the min and max of the original data\n",
    "min_value = np.min(data)\n",
    "max_value = np.max(data)\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaled_data = ((data - min_value) / (max_value - min_value)) * (new_max - new_min) + new_min\n",
    "\n",
    "print(scaled_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d9495d6-857f-4221-904f-846ff21cf980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratios: [9.19360488e-01 7.94897720e-02 1.14973970e-03 4.52647078e-35]\n",
      "Cumulative Explained Variance: [0.91936049 0.99885026 1.         1.        ]\n",
      "Number of Principal Components to Retain: 2\n"
     ]
    }
   ],
   "source": [
    "#Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "# Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "\"\"\"Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\"\"\"\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset with features: height, weight, age, gender, blood pressure\n",
    "data = np.array([[170, 70, 30, 1, 120],\n",
    "                 [160, 65, 25, 0, 130],\n",
    "                 [180, 80, 35, 1, 110],\n",
    "                 [155, 52, 28, 0, 125]])\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Fit the PCA model to your data\n",
    "pca.fit(data)\n",
    "\n",
    "# Calculate the explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Set a threshold, e.g., 95% variance\n",
    "threshold = 0.95\n",
    "\n",
    "# Determine the number of principal components to retain\n",
    "num_components_to_retain = np.argmax(cumulative_variance >= threshold) + 1\n",
    "\n",
    "print(\"Explained Variance Ratios:\", explained_variance_ratio)\n",
    "print(\"Cumulative Explained Variance:\", cumulative_variance)\n",
    "print(\"Number of Principal Components to Retain:\", num_components_to_retain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a086d-f862-4e53-8c5b-c480f59a051e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
